{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5ed449",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colrev.ops.dedupe_benchmark import DedupeBenchmarker\n",
    "from bib_dedupe.bib_dedupe import BibDeduper\n",
    "from bib_dedupe.util import BibDedupeUtil\n",
    "from asreview.data import load_data, ASReviewData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_util = BibDedupeUtil()\n",
    "\n",
    "for benchmark_path in [\"respiratory\", \"digital_work\", \"stroke\", \"haematology\", \"cytology_screening\"]:\n",
    "\n",
    "    dedupe_benchmark = DedupeBenchmarker(benchmark_path=f\"../tests/data/{benchmark_path}\")\n",
    "    records_df = dedupe_benchmark.get_records_for_dedupe()\n",
    "    \n",
    "    # Bib-dedupe\n",
    "    dedupe_instance = BibDeduper()\n",
    "    actual_blocked_df = dedupe_instance.block_pairs_for_deduplication(records_df=records_df)\n",
    "    matches = dedupe_instance.identify_true_matches(actual_blocked_df)\n",
    "    to_drop = [o for l in matches[\"duplicate_origin_sets\"] for o in l[1:]]\n",
    "    merged_df = records_df[~records_df['colrev_origin'].isin(to_drop)]\n",
    "    result = dedupe_benchmark.compare_dedupe_id(records_df=records_df, merged_df=merged_df)\n",
    "    \n",
    "    bd_util.append_to_output(result, package_name=\"bib-dedupe\")\n",
    "    \n",
    "    # More detailed comparison for debugging\n",
    "    results = dedupe_benchmark.compare(\n",
    "    blocked_df=actual_blocked_df,\n",
    "        predicted=matches['duplicate_origin_sets'],\n",
    "    )\n",
    "    dedupe_benchmark.export_cases(prepared_records_df=records_df, results=results)\n",
    "\n",
    "    # ASReview\n",
    "    asdata = ASReviewData(records_df)\n",
    "    merged_df = asdata.drop_duplicates()\n",
    "    result = dedupe_benchmark.compare_dedupe_id(records_df=records_df, merged_df=merged_df)\n",
    "\n",
    "    bd_util.append_to_output(result, package_name=\"asreview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73803013",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"../output/evaluation.csv\")\n",
    "grouped_df = results_df.groupby([\"package\", \"dataset\"], group_keys=True).apply(lambda x: x.sort_values(\"time\").tail(1)).reset_index(drop=True)\n",
    "\n",
    "datasets = grouped_df['dataset'].unique()\n",
    "\n",
    "for dataset in datasets:\n",
    "    plt.figure(figsize=(14, 3))\n",
    "    plt.suptitle(f\"Dataset: {dataset}\", fontsize=14, fontweight='bold')  # Added dataset as subheading title\n",
    "    ax1 = plt.subplot(121)\n",
    "    grouped_df[grouped_df['dataset'] == dataset].plot(ax=ax1, x=\"package\", y=\"false_positive_rate\", kind=\"barh\")\n",
    "    plt.title(f\"False positive rate by package\")\n",
    "    plt.legend().remove()\n",
    "    plt.ylabel(\"\")\n",
    "    for p in ax1.patches:\n",
    "        ax1.annotate(f\"{p.get_width():.2f}\", (p.get_width(), p.get_y() + p.get_height() / 2), ha='left', va='center')\n",
    "\n",
    "    ax2 = plt.subplot(122)\n",
    "    grouped_df[grouped_df['dataset'] == dataset].plot(ax=ax2, x=\"package\", y=\"sensitivity\", kind=\"barh\")\n",
    "    plt.title(f\"Sensitivity by package\")\n",
    "    plt.legend().remove()\n",
    "    plt.ylabel(\"\")\n",
    "    for p in ax2.patches:\n",
    "        ax2.annotate(f\"{p.get_width():.2f}\", (p.get_width(), p.get_y() + p.get_height() / 2), ha='left', va='center')\n",
    "\n",
    "    latest_time = results_df[\"time\"].max()\n",
    "    plt.figtext(0.5, 0.001, f\"Time of last evaluation run: {latest_time}\", ha='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig(str(Path(f\"../output/evaluation_{dataset}.png\")))\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
